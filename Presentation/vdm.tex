\section*{VDM's}
\begin{frame}{Variational Diffusion Models (VDM's)}
    Variational Diffusion Model (VDM) is simply a MHVAE with 3 restrictions:
    \begin{enumerate}
        \item The dimension of latent variables is equal to dimension of data.
        \item Distribution of latent variables at each time step is Gaussian Centered at latent variable at previous time step. (This is not learnt, but fixed)
        \item Encoding transitions are done in such a way that distribution of latent variable at last time step is standard Gaussian.
    \end{enumerate}

    Let us see what changes occur in MHVAE when these restrictions are applied to it.
\end{frame}

\begin{frame}{Variational Diffuion Models (VDM's)}
    \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{Images/vdm1.png}
        \caption{Variational Diffusion Model}
    \end{figure}
\end{frame}

\begin{frame}{Variational Diffusion Models (VDM's)}
    \begin{itemize}
        \item $x_0$ represents true data observations.
        \item $x_t$ represents intermediate noisy version of data.
        \item $x_T$ represents pure Gaussian noise.
    \end{itemize}
\end{frame}

\begin{frame}{Restriction 1}
    According to slight change of notation in previous slide, VDM's posterior is same as MHVAE's posterior which can now be written as:
    \begin{align*}
        q(x_{1:T}|x_0) &= \prod_{t=1}^{T} q(x_t|x_{t-1}) && \text{(VDM's Posterior)}
    \end{align*}
    instead of 
    \begin{align*}
        q(z_{1:T}|x) &= p(z_1|x) \prod_{t=2}^{T} q(z_t|z_{t-1}) && \text{(MHVAE's Posterior)}
    \end{align*}
\end{frame}

\begin{frame}{Restriction 2}
    Unlike MHVAE, distribution of latent variable at each time step is not learnt, but is fixed. Mathematically, the encoder transitions can be written as:
    \begin{align*}
        q(x_t|x_{t-1}) &= \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t) \mathbb{I}_{d \times d}) 
    \end{align*}

    Distribution of latent variable $x_t$ at each time step is gaussian centered at latent variable at previous time step $x_{t-1}$.
\end{frame}

\begin{frame}{Restriction 3}
    Gaussian Transitions are made in such a way that distribution of final latent variable $x_T$ is standard Gaussian. Mathematically, this can be written as:
    \begin{align*}
        q(x_T|x_{T-1}) &= \mathcal{N}(x_T; 0, \mathbb{I}_{d \times d})
    \end{align*}

    Hence Joint Distribution of VDM can be written as:
    \begin{align*}
        p(x_{0:T}) &= p(x_T) \prod_{t=1}^{T} q(x_{t-1}|x_{t}) && p(x_T) \sim \mathcal{N}(0, \mathbb{I}_{d \times d})
    \end{align*}
    Instead of
    \begin{align*}
        p(x,z_{1:T}) &= p(x_T)p(x|z_1) \prod_{t=2}^{T} q(z_{t-1}|z_{t}) && \text{(MHVAE's Joint)}
    \end{align*}
\end{frame}

\begin{frame}{Learning conditionals}
    Since encoder transitions are fixed for each time steps, we are only interested in learning conditionals $p(x_{t-1}|x_t)$ so that we can simulate new data. 

    \bigskip

    We can achieve this by maximizing ELBO for VDM. To show that maximizing ELBO actually helps in learning conditionals, we will first split ELBO in different components like we did in VAE and got the reconstruction term and prior matching term.
\end{frame}

\begin{frame}{Breaking down ELBO}
    We know that ELBO for MHVAE can be written as:
    \begin{align*}
        ELBO &=  \mathbb{E}_{q(z_{1:T}|x)} \left[ \log \left( \frac{p(x, z_{1:T})}{q(z_{1:T}|x)} \right) \right] \\
    \end{align*}
    According to notation's for VDM, we can write ELBO as:
    \begin{align*}
        ELBO &=  \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_{0:T})}{q(x_{1:T}|x_0)} \right) \right] \\
    \end{align*}
\end{frame}

\begin{frame}{Breaking down ELBO}
    We further saw that ELBO for MHVAE can be written as:
    \begin{align*}
        ELBO &= &= \mathbb{E}_{q(z_{1:T}|x)} \left[ \log \left( \frac{p(z_T)p(x|z_1) \prod_{t=2}^{T} p(z_{t-1}|z_t)}{q(z_1|x) \prod_{t=2}^{T} q(z_t|z_{t-1})} \right) \right] \\
    \end{align*}

    In our case we can write this as:
    \begin{align*}
        ELBO &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T) \prod_{t=1}^{T} p(x_{t-1}|x_t)}{\prod_{t=1}^{T} q(x_t|x_{t-1})} \right) \right] \\
    \end{align*}

    Let us now further simplify this expression.
\end{frame}

\begin{frame}{Breaking down ELBO}
    \begin{align*}
        ELBO &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T) \prod_{t=1}^{T} p(x_{t-1}|x_t)}{\prod_{t=1}^{T} q(x_t|x_{t-1})} \right) \right] \\
        &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T)p(x_0|x_1) \prod_{t=2}^{T} p(x_{t-1}|x_t)}{\prod_{t=1}^{T} q(x_t|x_{t-1})} \right) \right] \\
        &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T)p(x_0|x_1) \prod_{t=2}^{T} p(x_{t-1}|x_t)}{q(x_T|x_{T-1}) \prod_{t=1}^{T-1} q(x_t|x_{t-1})} \right) \right] \\
        &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T)p(x_0|x_1) \prod_{t=1}^{T-1} p(x_{t}|x_{t+1})}{q(x_T|x_{T-1}) \prod_{t=1}^{T-1} q(x_t|x_{t-1})} \right) \right] \\
    \end{align*}
\end{frame}

\begin{frame}{Breaking down ELBO}
    \begin{align*}
        &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T)p(x_0|x_1)}{q(x_T|x_{T-1})} \right) + \sum_{t=1}^{T-1} \log \left( \frac{p(x_{t}|x_{t+1})}{q(x_t|x_{t-1})} \right) \right] \\
        &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T)p(x_0|x_1)}{q(x_T|x_{T-1})} \right) \right] + \sum_{t=1}^{T-1} \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_{t}|x_{t+1})}{q(x_t|x_{t-1})} \right) \right] \\
    \end{align*}

    First Term can further be simplified into
    \begin{align*}
        &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T)}{q(x_T|x_{T-1})} \right) \right] + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( p(x_0|x_1) \right) \right] \\
    \end{align*}
\end{frame}

\begin{frame}{Reconstruction Term}
    We got 3 terms:
    \begin{enumerate}
        \item $\mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( p(x_0|x_1) \right) \right]$
    \end{enumerate}
    \bigskip
    Since inside expression is independent of $x_{2:T}$, we can write this as:
    \begin{align*}
        &= \mathbb{E}_{q(x_{1}|x_0)} \left[ \log \left( p(x_0|x_1) \right) \right] && \text{(\textcolor{red}{reconstruction term})}
    \end{align*}

    maximizing ELBO results in maximizing this term which is log likelihood of getting back original data sample from latent variable at first time step.
\end{frame}

\begin{frame}{Prior Matching Term}
    \begin{enumerate}
        \setcounter{enumi}{1}   
        \item $\mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_T)}{q(x_T|x_{T-1})} \right) \right]$
    \end{enumerate}
    We can simplify this expression as follows:
    \begin{align*}
        &= \mathbb{E}_{q(x_{T-1,T}|x_{0})} \left[ \log \left( \frac{p(x_T)}{q(x_T|x_{T-1})} \right) \right] \\
        &= \int q(x_{T-1,T}|x_{0}) \log \left( \frac{p(x_T)}{q(x_T|x_{T-1})} \right) dx_{T-1} dx_T \\
        &= \int q(x_{T-1}|x_{0}) \left[ q(x_T|x_{T-1}) \log \left( \frac{p(x_T)}{q(x_T|x_{T-1})} \right) dx_T \right] dx_{T-1} \\
        &= -\int q(x_{T-1}|x_{0}) D_{KL} \left( q(x_T|x_{T-1}) || p(x_T) \right) dx_{T-1} \\
        &= -\mathbb{E}_{q(x_{T-1}|x_{0})} \left[ D_{KL} \left( q(x_T|x_{T-1}) || p(x_T) \right) \right] 
    \end{align*} 
\end{frame}

\begin{frame}{Prior Matching Term}
    \begin{align*}
        &= -\mathbb{E}_{q(x_{T-1}|x_{0})} \left[ D_{KL} \left( q(x_T|x_{T-1}) || p(x_T) \right) \right] 
    \end{align*} 

    This is \textcolor{red}{prior matching term}, maximizing ELBO results in minimizing this term, which forces the distribution of latent variable at last time as close as possible to $p(x_T) \sim \mathcal{N}(0, \mathbb{I}_{d \times d})$.
\end{frame}

\begin{frame}{Consistency Term}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item $\mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \left( \frac{p(x_{t}|x_{t+1})}{q(x_t|x_{t-1})} \right) \right]$
    \end{enumerate}

    This term can be simplified as:
    \begin{align*}
        &= \mathbb{E}_{q(x_{t-1,t,t+1}|x_{0})} \left[ \log \left( \frac{p(x_{t}|x_{t+1})}{q(x_t|x_{t-1})} \right) \right] \\
        &= \int q(x_{t-1,t,t+1}|x_{0}) \log \left( \frac{p(x_{t}|x_{t+1})}{q(x_t|x_{t-1})} \right) dx_{t-1} dx_t dx_{t+1} \\
        &= \int q(x_{t+1}|x_{0}) q(x_{t-1}|x_{t+1}) \left[ q(x_{t}|x_{t-1}) \log \left( \frac{p(x_{t}|x_{t+1})}{q(x_t|x_{t-1})} \right) dx_t \right] dx_{t-1,t+1} \\
        &= -\int q(x_{t+1}|x_{0}) q(x_{t-1}|x_{t+1}) D_{KL} \left( q(x_t|x_{t-1}) || p(x_t|x_{t+1}) \right) dx_{t-1,t+1} \\
        &= -\mathbb{E}_{q(x_{t+1}|x_{0}) q(x_{t-1}|x_{t+1})} \left[ D_{KL} \left( q(x_t|x_{t-1}) || p(x_t|x_{t+1}) \right) \right]
    \end{align*}
\end{frame}

\begin{frame}{Consistency Term}
    \begin{align*}
        &= -\mathbb{E}_{q(x_{t-1,t+1}|x_{0})} \left[ D_{KL} \left( q(x_t|x_{t-1}) || p(x_t|x_{t+1}) \right) \right]
    \end{align*}

    This is \textcolor{red}{consistency term}, maximizing ELBO results in minimizing this term, which ensures that distribution of noised image at time step $t$ i.e $q(x_t|x_{t-1})$ is as close as possible to distribution of denoised image at time step $t$ i.e $p(x_t|x_{t+1})$.
    
\end{frame}

\begin{frame}{ELBO}
    Finally we saw that ELBO = reconstruction term + prior matching term + consistency term where:
    \begin{enumerate}
        \item Reconstruction term $\mathbb{E}_{q(x_{1}|x_0)} \left[ \log \left( p(x_0|x_1) \right) \right]$
        \item Prior Matching term $-\mathbb{E}_{q(x_{T-1}|x_{0})} \left[ D_{KL} \left( q(x_T|x_{T-1}) || p(x_T) \right) \right]$
        \item Consistency term $-\mathbb{E}_{q(x_{t-1,t+1}|x_{0})} \left[ D_{KL} \left( q(x_t|x_{t-1}) || p(x_t|x_{t+1}) \right) \right]$
    \end{enumerate}
    \bigskip
    \textcolor{red}{maximizing ELBO results in minimizing prior matching term and consistency term and maximizing reconstruction term.}
\end{frame}